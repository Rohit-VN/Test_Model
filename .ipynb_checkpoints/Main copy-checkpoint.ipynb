{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "a77f51aa-0530-4641-b0ff-87a268b4a591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import re\n",
    "from typing import Union, Optional, List, Dict, Any, Tuple, Set\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(\n",
    "    'test_data.csv',\n",
    "    parse_dates=['date_added', 'date_modified']\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "a6eab916-8a63-4e82-9f92-446b1c04defd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_price_range_filter(df: pd.DataFrame, price_col: str, **price_kwargs) -> pd.DataFrame:\n",
    "   \n",
    "    if price_col not in df.columns:\n",
    "        print(f\"Warning: Price column '{price_col}' not found!\")\n",
    "        return df.copy()\n",
    "\n",
    "    mask = pd.Series(True, index=df.index)\n",
    "\n",
    "    if 'price_gt' in price_kwargs:\n",
    "        mask &= df[price_col] > price_kwargs['price_gt']\n",
    "        print(f\"Applied price_gt filter: > {price_kwargs['price_gt']}\")\n",
    "\n",
    "    if 'price_gte' in price_kwargs:\n",
    "        mask &= df[price_col] >= price_kwargs['price_gte']\n",
    "        print(f\"Applied price_gte filter: >= {price_kwargs['price_gte']}\")\n",
    "\n",
    "    if 'price_lt' in price_kwargs:\n",
    "        mask &= df[price_col] < price_kwargs['price_lt']\n",
    "        print(f\"Applied price_lt filter: < {price_kwargs['price_lt']}\")\n",
    "\n",
    "    if 'price_lte' in price_kwargs:\n",
    "        mask &= df[price_col] <= price_kwargs['price_lte']\n",
    "        print(f\"Applied price_lte filter: <= {price_kwargs['price_lte']}\")\n",
    "\n",
    "    if 'price_between' in price_kwargs:\n",
    "        min_val, max_val = price_kwargs['price_between']\n",
    "        mask &= (df[price_col] >= min_val) & (df[price_col] <= max_val)\n",
    "        print(f\"Applied price_between filter: {min_val} <= price <= {max_val}\")\n",
    "\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfc7a09-93e3-4741-9c61-9c0a626f50f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "def apply_date_range_filter(\n",
    "    df: pd.DataFrame,\n",
    "    date_col: str,\n",
    "    handle_nulls: str = 'exclude',\n",
    "    timezone: Optional[str] = None,\n",
    "    date_format: Optional[str] = None,\n",
    "    verbose: bool = False,\n",
    "    *,\n",
    "    date_from: Optional[str] = None,\n",
    "    date_to: Optional[str] = None,\n",
    "    date_after: Optional[str] = None,\n",
    "    date_before: Optional[str] = None,\n",
    "    date_between: Optional[Tuple[str, str]] = None,\n",
    "    days_ago: Optional[int] = None,\n",
    "    date_today: Optional[bool] = None\n",
    ") -> pd.Series:\n",
    "    \n",
    "    # --- Example simplified implementation ---\n",
    "\n",
    "    # Defensive checks (you can expand this)\n",
    "    if date_col not in df.columns:\n",
    "        raise KeyError(f\"Column '{date_col}' not found in DataFrame.\")\n",
    "\n",
    "    date_series = df[date_col]\n",
    "\n",
    "    # Convert to datetime if not already\n",
    "    if not pd.api.types.is_datetime64_any_dtype(date_series):\n",
    "        date_series = pd.to_datetime(date_series, format=date_format, errors='coerce')\n",
    "\n",
    "    # Handle nulls\n",
    "    if handle_nulls == 'exclude':\n",
    "        mask = ~date_series.isna()\n",
    "    elif handle_nulls == 'include':\n",
    "        mask = pd.Series(True, index=df.index)\n",
    "    elif handle_nulls == 'error' and date_series.isna().any():\n",
    "        raise ValueError(f\"Null values found in '{date_col}'\")\n",
    "\n",
    "    else:\n",
    "        mask = pd.Series(True, index=df.index)\n",
    "\n",
    "    # Apply filters one by one (only if they exist)\n",
    "    if date_from:\n",
    "        mask &= date_series >= pd.to_datetime(date_from)\n",
    "    if date_to:\n",
    "        mask &= date_series <= pd.to_datetime(date_to)\n",
    "    if date_after:\n",
    "        mask &= date_series > pd.to_datetime(date_after)\n",
    "    if date_before:\n",
    "        mask &= date_series < pd.to_datetime(date_before)\n",
    "    if date_between:\n",
    "        start, end = date_between\n",
    "        mask &= (date_series >= pd.to_datetime(start)) & (date_series <= pd.to_datetime(end))\n",
    "    if days_ago is not None:\n",
    "        cutoff = pd.Timestamp.now(tz=timezone) - timedelta(days=days_ago)\n",
    "        mask &= date_series >= cutoff\n",
    "    if date_today:\n",
    "        today = pd.Timestamp.now(tz=timezone).normalize()\n",
    "        tomorrow = today + timedelta(days=1)\n",
    "        mask &= (date_series >= today) & (date_series < tomorrow)\n",
    "\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "9690c9cd-a606-44fc-9a25-d12169c8774b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_range_parameters(kwargs: dict) -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Separate range parameters from regular filter parameters.\n",
    "    \n",
    "    Args:\n",
    "        kwargs: Dictionary of filter parameters\n",
    "    \n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "        - price_params: Dictionary of price range parameters\n",
    "        - date_params: Dictionary of date range parameters  \n",
    "        - regular_params: Dictionary of regular filter parameters\n",
    "    \n",
    "    Possible Outputs and Scenarios:\n",
    "    \n",
    "    1. ALL EMPTY DICTIONARIES:\n",
    "       - Input: {} or None values only\n",
    "       - Output: ({}, {}, {})\n",
    "    \n",
    "    2. ONLY PRICE PARAMETERS:\n",
    "       - Input: {'price_min': 1000, 'price_max': 5000}\n",
    "       - Output: ({'price_min': 1000, 'price_max': 5000}, {}, {})\n",
    "    \n",
    "    3. ONLY DATE PARAMETERS:\n",
    "       - Input: {'date_from': '2025-01-01', 'date_added_min': '2025-02-01'}\n",
    "       - Output: ({}, {'date_from': '2025-01-01', 'date_added_min': '2025-02-01'}, {})\n",
    "    \n",
    "    4. ONLY REGULAR PARAMETERS:\n",
    "       - Input: {'brand': 'Teledyne', 'category_id': 5}\n",
    "       - Output: ({}, {}, {'brand': 'Teledyne', 'category_id': 5})\n",
    "    \n",
    "    5. MIXED PARAMETERS:\n",
    "       - Input: {'price_min': 1000, 'date_from': '2025-01-01', 'brand': 'Teledyne'}\n",
    "       - Output: ({'price_min': 1000}, {'date_from': '2025-01-01'}, {'brand': 'Teledyne'})\n",
    "    \n",
    "    6. EDGE CASES:\n",
    "       - None values: Preserved in their respective categories\n",
    "       - Empty strings: Preserved in their respective categories\n",
    "       - Case sensitivity: Exact match required\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handle edge case: None input\n",
    "    if kwargs is None:\n",
    "        return {}, {}, {}\n",
    "    \n",
    "    # Handle edge case: empty dict\n",
    "    if not kwargs:\n",
    "        return {}, {}, {}\n",
    "    \n",
    "    price_prefixes = {\n",
    "        'price_min', 'price_max', 'price_gt', 'price_gte', \n",
    "        'price_lt', 'price_lte', 'price_between'\n",
    "    }\n",
    "    \n",
    "    date_prefixes = {\n",
    "        'date_from', 'date_to', 'date_after', 'date_before', \n",
    "        'date_between', 'days_ago', 'date_today'\n",
    "    }\n",
    "    \n",
    "    # Column-specific date range patterns\n",
    "    date_column_patterns = ['date_added_', 'date_modified_']\n",
    "    \n",
    "    price_params = {}\n",
    "    date_params = {}\n",
    "    regular_params = {}\n",
    "    \n",
    "    for key, value in kwargs.items():\n",
    "        # Convert key to string to handle potential non-string keys\n",
    "        key_str = str(key)\n",
    "        \n",
    "        if key_str in price_prefixes:\n",
    "            price_params[key] = value\n",
    "        elif key_str in date_prefixes:\n",
    "            date_params[key] = value\n",
    "        elif any(key_str.startswith(pattern) for pattern in date_column_patterns):\n",
    "            # Handle column-specific date ranges like date_added_from, date_modified_to\n",
    "            date_params[key] = value\n",
    "        else:\n",
    "            regular_params[key] = value\n",
    "    \n",
    "    return price_params, date_params, regular_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "fa453b0e-f19b-44b7-8d8b-a889de5d0779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the filter implementation here\n",
    "def apply_regular_filters(df: pd.DataFrame, **kwargs) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Apply regular filtering logic with improved error handling and edge case management.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        **kwargs: Filter conditions where key=column_name, value=filter_value\n",
    "        \n",
    "    Returns:\n",
    "        pd.Series: Boolean mask for filtering\n",
    "        \n",
    "    Features:\n",
    "        - Case-insensitive string matching for object columns\n",
    "        - Partial matching for strings\n",
    "        - Multiple value support (OR logic)\n",
    "        - Exact matching for numeric columns\n",
    "        - Proper handling of None/NaN values\n",
    "        - No modification of original DataFrame\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"Warning: DataFrame is empty\")\n",
    "        return pd.Series(dtype=bool, index=df.index, name='empty_mask')\n",
    "\n",
    "    if not kwargs:\n",
    "        print(\"No filter conditions provided\")\n",
    "        return pd.Series(True, index=df.index, name='no_filter_mask')\n",
    "\n",
    "    condition_masks = []\n",
    "    for col, value in kwargs.items():\n",
    "        if col not in df.columns:\n",
    "            print(f\"Warning: Column '{col}' not found in DataFrame. Available columns: {list(df.columns)}\")\n",
    "            continue\n",
    "\n",
    "        if df[col].isnull().all():\n",
    "            print(f\"Warning: Column '{col}' contains only NaN values. Skipping condition.\")\n",
    "            continue\n",
    "\n",
    "        condition_mask = _process_single_condition(df, col, value)\n",
    "        if condition_mask is not None:\n",
    "            condition_masks.append(condition_mask)\n",
    "\n",
    "    if not condition_masks:\n",
    "        print(\"Warning: No valid filter conditions found. Returning empty result.\")\n",
    "        return pd.Series(False, index=df.index, name='no_valid_conditions')\n",
    "\n",
    "    final_mask = condition_masks[0]\n",
    "    for mask in condition_masks[1:]:\n",
    "        final_mask = final_mask & mask\n",
    "\n",
    "    matches = final_mask.sum()\n",
    "    print(f\"Filter applied: {matches} out of {len(df)} rows match all conditions\")\n",
    "    return final_mask\n",
    "\n",
    "def _process_single_condition(df: pd.DataFrame, col: str, value: Any) -> Union[pd.Series, None]:\n",
    "    \"\"\"Process a single filter condition.\"\"\"\n",
    "    col_dtype = df[col].dtype\n",
    "\n",
    "    if value is None or (isinstance(value, float) and pd.isna(value)):\n",
    "        return df[col].isna()\n",
    "\n",
    "    if isinstance(value, (list, tuple, set)):\n",
    "        return _handle_multiple_values(df, col, value, col_dtype)\n",
    "\n",
    "    return _handle_single_value(df, col, value, col_dtype)\n",
    "\n",
    "def _handle_multiple_values(df: pd.DataFrame, col: str, values: Union[List, Tuple, Set], col_dtype) -> Union[pd.Series, None]:\n",
    "    \"\"\"Handle multiple values with OR logic.\"\"\"\n",
    "    non_none_values = [v for v in values if v is not None and not (isinstance(v, float) and pd.isna(v))]\n",
    "    none_values = [v for v in values if v is None or (isinstance(v, float) and pd.isna(v))]\n",
    "\n",
    "    masks = []\n",
    "\n",
    "    if non_none_values:\n",
    "        if pd.api.types.is_object_dtype(col_dtype):\n",
    "            value_masks = [_handle_single_value(df, col, v, col_dtype) for v in non_none_values]\n",
    "            value_masks = [m for m in value_masks if m is not None]\n",
    "            if value_masks:\n",
    "                combined_mask = value_masks[0]\n",
    "                for mask in value_masks[1:]:\n",
    "                    combined_mask = combined_mask | mask\n",
    "                masks.append(combined_mask)\n",
    "        else:\n",
    "            masks.append(df[col].isin(non_none_values))\n",
    "\n",
    "    if none_values:\n",
    "        masks.append(df[col].isna())\n",
    "\n",
    "    if not masks:\n",
    "        print(f\"Warning: No valid values found in list for column '{col}'\")\n",
    "        return None\n",
    "\n",
    "    final_mask = masks[0]\n",
    "    for mask in masks[1:]:\n",
    "        final_mask = final_mask | mask\n",
    "\n",
    "    return final_mask\n",
    "\n",
    "def _handle_single_value(df: pd.DataFrame, col: str, value: Any, col_dtype) -> Union[pd.Series, None]:\n",
    "    \"\"\"Handle a single value.\"\"\"\n",
    "    if pd.api.types.is_object_dtype(col_dtype):\n",
    "        working_series = df[col].fillna('').astype(str).str.lower()\n",
    "        escaped_value = re.escape(str(value).lower())\n",
    "        mask = working_series.str.contains(escaped_value, na=False, regex=True)\n",
    "        if not mask.any():\n",
    "            print(f\"Warning: Value '{value}' not found in column '{col}'. No matches.\")\n",
    "            return pd.Series(False, index=df.index)\n",
    "        return mask\n",
    "    else:\n",
    "        try:\n",
    "            if pd.api.types.is_numeric_dtype(col_dtype):\n",
    "                converted_value = pd.to_numeric(value, errors='coerce')\n",
    "                if pd.isna(converted_value):\n",
    "                    print(f\"Warning: Value '{value}' cannot be converted to numeric for column '{col}'\")\n",
    "                    return pd.Series(False, index=df.index)\n",
    "                mask = df[col] == converted_value\n",
    "            else:\n",
    "                mask = df[col] == value\n",
    "\n",
    "            if not mask.any():\n",
    "                print(f\"Warning: Value '{value}' not found in column '{col}'. No matches.\")\n",
    "                return pd.Series(False, index=df.index)\n",
    "\n",
    "            return mask\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error processing value '{value}' for column '{col}': {str(e)}\")\n",
    "            return pd.Series(False, index=df.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "63f024d9-ae13-46b7-914d-e8429bb40c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_products_enhanced(df: pd.DataFrame, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter products in the DataFrame using combined price range, date range, and regular filters.\n",
    "\n",
    "    This function applies advanced filtering logic to the input DataFrame:\n",
    "    - Price filters on the 'price' column (min/max, greater than, less than).\n",
    "    - Date filters on 'date_added' and 'date_modified' columns, with optional prefixes:\n",
    "      - General date filters apply to 'date_added'.\n",
    "      - Prefixed filters apply specifically to 'date_added' or 'date_modified'.\n",
    "    - Regular filters on specific DataFrame columns such as 'brand', 'category_id', etc.\n",
    "\n",
    "    The filters are combined with AND logic, meaning rows must satisfy all provided conditions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame containing product data.\n",
    "\n",
    "    **kwargs : dict\n",
    "        Filter parameters which may include:\n",
    "\n",
    "        Price filters (all optional):\n",
    "            - price_min (float): Minimum price (inclusive).\n",
    "            - price_max (float): Maximum price (inclusive).\n",
    "            - price_gt (float): Price strictly greater than this value.\n",
    "            - price_gte (float): Price greater than or equal to this value.\n",
    "            - price_lt (float): Price strictly less than this value.\n",
    "            - price_lte (float): Price less than or equal to this value.\n",
    "\n",
    "        Date filters on 'date_added' and 'date_modified' (all optional):\n",
    "            - date_from (str): Minimum date (inclusive) for 'date_added'.\n",
    "            - date_to (str): Maximum date (inclusive) for 'date_added'.\n",
    "            - date_after (str): Date strictly after (exclusive) for 'date_added'.\n",
    "            - date_before (str): Date strictly before (exclusive) for 'date_added'.\n",
    "            - date_added_min (str): Minimum date for 'date_added' column.\n",
    "            - date_added_max (str): Maximum date for 'date_added' column.\n",
    "            - date_modified_min (str): Minimum date for 'date_modified' column.\n",
    "            - date_modified_max (str): Maximum date for 'date_modified' column.\n",
    "\n",
    "        Regular filters (example keys, adjust as needed):\n",
    "            - brand (str): Case-insensitive partial string match on brand.\n",
    "            - category_id (int): Exact match on category ID.\n",
    "            - color (str): Case-insensitive partial string match on color.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Filtered DataFrame containing only rows that match all given filter criteria.\n",
    "\n",
    "    Warnings\n",
    "    --------\n",
    "    - If the DataFrame is empty or no filter kwargs are provided, the full DataFrame is returned.\n",
    "    - Invalid filter keys that do not correspond to DataFrame columns are ignored with a warning.\n",
    "    - If no rows match the filter criteria, an empty DataFrame is returned with a warning.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> filter_products_enhanced(df, price_min=100, price_max=500, brand='sony')\n",
    "    >>> filter_products_enhanced(df, date_added_min='2025-01-01', category_id=12)\n",
    "    >>> filter_products_enhanced(df, price_gt=50, date_from='2025-03-01', color='red')\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"Warning: The DataFrame is empty.\")\n",
    "        return df.copy()\n",
    "    \n",
    "    if not kwargs:\n",
    "        print(\"No filter conditions provided. Returning full DataFrame.\")\n",
    "        return df.copy()\n",
    "    \n",
    "    working_df = df.copy()\n",
    "    \n",
    "    price_params, date_params, regular_params = detect_range_parameters(kwargs)\n",
    "    \n",
    "    final_mask = pd.Series(True, index=working_df.index)\n",
    "    \n",
    "    if price_params:\n",
    "        price_mask = apply_price_range_filter(working_df, 'price', **price_params)\n",
    "        final_mask &= price_mask\n",
    "    \n",
    "    if date_params:\n",
    "        general_date_params = {k: v for k, v in date_params.items() \n",
    "                               if not k.startswith(('date_added_', 'date_modified_'))}\n",
    "        date_added_params = {k.replace('date_added_', ''): v for k, v in date_params.items() \n",
    "                            if k.startswith('date_added_')}\n",
    "        date_modified_params = {k.replace('date_modified_', ''): v for k, v in date_params.items() \n",
    "                               if k.startswith('date_modified_')}\n",
    "        \n",
    "        if general_date_params:\n",
    "            date_mask = apply_date_range_filter(working_df, 'date_added', **general_date_params)\n",
    "            final_mask &= date_mask\n",
    "        \n",
    "        if date_added_params:\n",
    "            date_mask = apply_date_range_filter(working_df, 'date_added', **date_added_params)\n",
    "            final_mask &= date_mask\n",
    "        \n",
    "        if date_modified_params:\n",
    "            date_mask = apply_date_range_filter(working_df, 'date_modified', **date_modified_params)\n",
    "            final_mask &= date_mask\n",
    "    \n",
    "    # Remove keys from regular_params that do not exist in DataFrame columns\n",
    "    valid_regular_params = {k: v for k, v in regular_params.items() if k in working_df.columns}\n",
    "    invalid_keys = set(regular_params.keys()) - set(valid_regular_params.keys())\n",
    "    if invalid_keys:\n",
    "        print(f\"Warning: The following filter columns do not exist and will be ignored: {invalid_keys}\")\n",
    "    \n",
    "    if valid_regular_params:\n",
    "        regular_mask = apply_regular_filters(working_df, **valid_regular_params)\n",
    "        final_mask &= regular_mask\n",
    "    \n",
    "    result_df = working_df[final_mask]\n",
    "    \n",
    "    if result_df.empty:\n",
    "        print(\"Warning: No rows match the given filter conditions.\")\n",
    "    \n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "ab82d3b6-81bc-42cf-817d-c537d1787350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_data(df: pd.DataFrame, \n",
    "                 group_by_column: Optional[Union[str, List[str]]] = None,\n",
    "                 agg_dict: Optional[Dict[str, Union[str, List[str]]]] = None,\n",
    "                 count_column: Optional[str] = None,\n",
    "                 include_group_size: bool = False,\n",
    "                 convert_numeric: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform grouped or ungrouped analysis on a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame to analyze\n",
    "    group_by_column : str, list of str, or None\n",
    "        Column(s) to group by. If None, performs ungrouped analysis.\n",
    "    agg_dict : dict or None\n",
    "        Dictionary mapping column names to aggregation functions.\n",
    "        If None and count_column is None, returns descriptive statistics.\n",
    "    count_column : str or None\n",
    "        Column to perform value counts on within each group.\n",
    "    include_group_size : bool\n",
    "        Whether to include group size in the output (only for grouped analysis).\n",
    "    convert_numeric : bool\n",
    "        Whether to attempt converting string columns to numeric if they contain numeric values.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Analysis results with consistent structure.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input validation\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError(\"Input must be a pandas DataFrame.\")\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"Warning: Input DataFrame is empty.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Convert numeric strings to numeric if requested\n",
    "    if convert_numeric:\n",
    "        df = _convert_numeric_strings(df)\n",
    "    \n",
    "    # Validate mutually exclusive parameters\n",
    "    if agg_dict is not None and count_column is not None:\n",
    "        raise ValueError(\"Cannot specify both agg_dict and count_column. Choose one.\")\n",
    "    \n",
    "    # Determine if this is grouped or ungrouped analysis\n",
    "    is_grouped = group_by_column is not None\n",
    "    \n",
    "    if is_grouped:\n",
    "        return _grouped_analysis(df, group_by_column, agg_dict, count_column, include_group_size)\n",
    "    else:\n",
    "        return _ungrouped_analysis(df, agg_dict, count_column)\n",
    "\n",
    "\n",
    "def _grouped_analysis(df: pd.DataFrame, \n",
    "                      group_by_column: Union[str, List[str]],\n",
    "                      agg_dict: Optional[Dict[str, Union[str, List[str]]]],\n",
    "                      count_column: Optional[str],\n",
    "                      include_group_size: bool) -> pd.DataFrame:\n",
    "    # Normalize group_by_column to list\n",
    "    if isinstance(group_by_column, str):\n",
    "        group_by_column = [group_by_column]\n",
    "\n",
    "    # Validate grouping columns\n",
    "    if not all(col in df.columns for col in group_by_column):\n",
    "        missing = [col for col in group_by_column if col not in df.columns]\n",
    "        raise ValueError(f\"Grouping columns not found in DataFrame: {missing}\")\n",
    "\n",
    "    if df[group_by_column].isnull().any().any():\n",
    "        print(\"Warning: Missing values found in grouping columns. Dropping them.\")\n",
    "        df = df.dropna(subset=group_by_column)\n",
    "        if df.empty:\n",
    "            print(\"Warning: No data remaining after dropping missing group values.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        grouped = df.groupby(group_by_column, dropna=False)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error creating groups: {str(e)}\")\n",
    "\n",
    "    if agg_dict:\n",
    "        result = _apply_aggregations(grouped, agg_dict, df.columns)\n",
    "    elif count_column:\n",
    "        result = _apply_value_counts(grouped, count_column, df.columns, group_by_column)\n",
    "    else:\n",
    "        # ✅ NEW: Just return the raw grouped rows (i.e. original DataFrame)\n",
    "        return df\n",
    "\n",
    "    if include_group_size and not result.empty:\n",
    "        group_sizes = grouped.size().reset_index(name='group_size')\n",
    "        result = pd.merge(result, group_sizes, on=group_by_column, how='left')\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def _convert_numeric_strings(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert string columns to numeric if they contain numeric values.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            # Try to convert to numeric\n",
    "            try:\n",
    "                # Remove any whitespace first\n",
    "                if isinstance(df[col].iloc[0], str):\n",
    "                    df[col] = df[col].str.strip()\n",
    "                \n",
    "                # Try converting to numeric\n",
    "                numeric_series = pd.to_numeric(df[col], errors='coerce')\n",
    "                \n",
    "                # If most values converted successfully, use the numeric version\n",
    "                if numeric_series.notna().sum() > len(df) * 0.5:  # At least 50% are numeric\n",
    "                    df[col] = numeric_series\n",
    "                    print(f\"Converted column '{col}' from string to numeric.\")\n",
    "                    \n",
    "            except (AttributeError, ValueError):\n",
    "                # If conversion fails, keep as string\n",
    "                continue\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def _ungrouped_analysis(df: pd.DataFrame,\n",
    "                       agg_dict: Optional[Dict[str, Union[str, List[str]]]],\n",
    "                       count_column: Optional[str]) -> pd.DataFrame:\n",
    "    \"\"\"Internal function for ungrouped analysis.\"\"\"\n",
    "    \n",
    "    if agg_dict:\n",
    "        # Validate columns in agg_dict\n",
    "        for col in agg_dict.keys():\n",
    "            if col not in df.columns:\n",
    "                raise ValueError(f\"Column '{col}' in agg_dict not found in DataFrame.\")\n",
    "        \n",
    "        result = _apply_ungrouped_aggregations(df, agg_dict)\n",
    "    elif count_column:\n",
    "        if count_column not in df.columns:\n",
    "            raise ValueError(f\"Count column '{count_column}' not found in DataFrame.\")\n",
    "        \n",
    "        result = df[count_column].value_counts().reset_index()\n",
    "        result.columns = [count_column, 'count']\n",
    "    else:\n",
    "        # Default: descriptive statistics for numeric columns\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        if len(numeric_cols) == 0:\n",
    "            print(\"Warning: No numeric columns found for default analysis.\")\n",
    "            return pd.DataFrame({'message': ['No numeric columns available for analysis']})\n",
    "        \n",
    "        result = df[numeric_cols].describe().T.reset_index()\n",
    "        result.rename(columns={'index': 'column'}, inplace=True)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def _apply_aggregations(grouped, agg_dict: Dict[str, Union[str, List[str]]], df_columns) -> pd.DataFrame:\n",
    "    \"\"\"Apply aggregations to grouped data.\"\"\"\n",
    "    \n",
    "    # Validate columns exist\n",
    "    for col in agg_dict.keys():\n",
    "        if col not in df_columns:\n",
    "            raise ValueError(f\"Column '{col}' in agg_dict not found in DataFrame.\")\n",
    "    \n",
    "    # Validate aggregation functions\n",
    "    valid_aggs = ['mean', 'median', 'sum', 'count', 'std', 'var', 'min', 'max', 'first', 'last', 'nunique']\n",
    "    \n",
    "    for col, aggs in agg_dict.items():\n",
    "        # Handle different types of aggregation specifications\n",
    "        if isinstance(aggs, str):\n",
    "            # Single string aggregation\n",
    "            if aggs not in valid_aggs:\n",
    "                raise ValueError(f\"Invalid aggregation function: {aggs}\")\n",
    "        elif callable(aggs):\n",
    "            # Single callable function - skip validation, pandas will handle it\n",
    "            continue\n",
    "        elif isinstance(aggs, list):\n",
    "            # List of aggregations\n",
    "            for agg in aggs:\n",
    "                if isinstance(agg, str) and agg not in valid_aggs:\n",
    "                    raise ValueError(f\"Invalid aggregation function: {agg}\")\n",
    "                elif not isinstance(agg, str) and not callable(agg):\n",
    "                    raise ValueError(f\"Aggregation function must be a string or callable: {agg}\")\n",
    "        else:\n",
    "            # Single non-callable, non-string, non-list item\n",
    "            if not callable(aggs):\n",
    "                raise ValueError(f\"Aggregation function must be a string or callable: {aggs}\")\n",
    "    \n",
    "    try:\n",
    "        # Process the aggregation dictionary to handle custom functions\n",
    "        processed_agg_dict = {}\n",
    "        for col, aggs in agg_dict.items():\n",
    "            if isinstance(aggs, str):\n",
    "                processed_agg_dict[col] = aggs\n",
    "            elif callable(aggs):\n",
    "                # Single callable function\n",
    "                processed_agg_dict[col] = aggs\n",
    "            elif isinstance(aggs, list):\n",
    "                # List of functions\n",
    "                processed_agg_dict[col] = aggs\n",
    "            else:\n",
    "                # Single non-callable, non-string item\n",
    "                processed_agg_dict[col] = aggs\n",
    "        \n",
    "        result = grouped.agg(processed_agg_dict).reset_index()\n",
    "        \n",
    "        # Flatten column names if multi-level\n",
    "        if isinstance(result.columns, pd.MultiIndex):\n",
    "            new_columns = []\n",
    "            for col in result.columns:\n",
    "                if isinstance(col, tuple):\n",
    "                    if col[1] == '' or col[1] is None:\n",
    "                        new_columns.append(col[0])\n",
    "                    else:\n",
    "                        new_columns.append(f\"{col[0]}_{col[1]}\")\n",
    "                else:\n",
    "                    new_columns.append(col)\n",
    "            result.columns = new_columns\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error applying aggregations: {str(e)}\")\n",
    "\n",
    "\n",
    "def _apply_value_counts(grouped, count_column: str, df_columns, group_by_column: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Apply value counts to grouped data.\"\"\"\n",
    "    \n",
    "    if count_column not in df_columns:\n",
    "        raise ValueError(f\"Count column '{count_column}' not found in DataFrame.\")\n",
    "    \n",
    "    try:\n",
    "        result = grouped[count_column].value_counts().reset_index()\n",
    "        # Reorder columns to put grouping columns first\n",
    "        cols = group_by_column + [count_column, 'count']\n",
    "        result = result[cols]\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error applying value counts: {str(e)}\")\n",
    "\n",
    "\n",
    "def _apply_default_aggregations(grouped, df: pd.DataFrame, group_by_column: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Apply default aggregations to numeric columns.\"\"\"\n",
    "    \n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    numeric_cols = [col for col in numeric_cols if col not in group_by_column]\n",
    "    \n",
    "    if len(numeric_cols) == 0:\n",
    "        print(\"Warning: No numeric columns found for default analysis.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        result = grouped[numeric_cols].agg(['count', 'mean', 'std', 'min', 'max']).reset_index()\n",
    "        \n",
    "        # Flatten column names if multi-level\n",
    "        if isinstance(result.columns, pd.MultiIndex):\n",
    "            new_columns = []\n",
    "            for col in result.columns:\n",
    "                if isinstance(col, tuple):\n",
    "                    if col[1] == '' or col[1] is None:\n",
    "                        new_columns.append(col[0])\n",
    "                    else:\n",
    "                        new_columns.append(f\"{col[0]}_{col[1]}\")\n",
    "                else:\n",
    "                    new_columns.append(col)\n",
    "            result.columns = new_columns\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error applying default aggregations: {str(e)}\")\n",
    "\n",
    "def _apply_ungrouped_aggregations(df: pd.DataFrame, agg_dict: Dict[str, Union[str, List[Union[str, Any]], Any]]) -> pd.DataFrame:\n",
    "    \"\"\"Apply aggregations to ungrouped data.\"\"\"\n",
    "\n",
    "    try:\n",
    "        result_dict = {}\n",
    "\n",
    "        for col, aggs in agg_dict.items():\n",
    "            if isinstance(aggs, str) or callable(aggs):\n",
    "                aggs = [aggs]\n",
    "\n",
    "            for agg in aggs:\n",
    "                if agg == 'count':\n",
    "                    result_dict[f\"{col}_{agg}\"] = df[col].count()\n",
    "                elif agg == 'nunique':\n",
    "                    result_dict[f\"{col}_{agg}\"] = df[col].nunique()\n",
    "                elif agg in ['mean', 'median', 'sum', 'std', 'var', 'min', 'max']:\n",
    "                    if df[col].dtype in ['object', 'string']:\n",
    "                        print(f\"Warning: Cannot apply {agg} to non-numeric column '{col}'. Skipping.\")\n",
    "                        continue\n",
    "                    try:\n",
    "                        result_dict[f\"{col}_{agg}\"] = getattr(df[col], agg)()\n",
    "                    except Exception:\n",
    "                        print(f\"Warning: Could not apply {agg} to column '{col}'. Skipping.\")\n",
    "                        continue\n",
    "                elif agg in ['first', 'last']:\n",
    "                    try:\n",
    "                        result_dict[f\"{col}_{agg}\"] = getattr(df[col], agg)()\n",
    "                    except Exception:\n",
    "                        print(f\"Warning: Could not apply {agg} to column '{col}'. Skipping.\")\n",
    "                        continue\n",
    "                elif callable(agg):\n",
    "                    try:\n",
    "                        result_dict[f\"{col}_custom\"] = agg(df[col])\n",
    "                    except Exception:\n",
    "                        print(f\"Warning: Could not apply custom function to column '{col}'. Skipping.\")\n",
    "                        continue\n",
    "                else:\n",
    "                    raise ValueError(f\"Invalid aggregation function: {agg}\")\n",
    "\n",
    "        return pd.DataFrame([result_dict])\n",
    "\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error applying ungrouped aggregations: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "46d89c5d-bca7-430c-86f0-79f1430a41cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_values(\n",
    "    df: pd.DataFrame,\n",
    "    columns: Union[str, List[str]],\n",
    "    dropna: bool = False,\n",
    "    as_list: bool = False\n",
    ") -> Union[pd.Series, List, Dict[str, Union[pd.Series, List]]]:\n",
    "    \"\"\"\n",
    "    Get unique values from one or more DataFrame columns.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        columns (str or list of str): Column(s) to get unique values from.\n",
    "        dropna (bool): Whether to drop NaN values.\n",
    "        as_list (bool): If True, returns list(s) instead of Series.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series, list, or dict: Unique values.\n",
    "    \"\"\"\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError(\"'df' must be a pandas DataFrame.\")\n",
    "    \n",
    "    if isinstance(columns, str):\n",
    "        columns = [columns]\n",
    "    elif not isinstance(columns, list) or not all(isinstance(col, str) for col in columns):\n",
    "        raise TypeError(\"'columns' must be a string or list of strings.\")\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(\"Input DataFrame is empty.\")\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    for col in columns:\n",
    "        col = col.strip()\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Column '{col}' not found in DataFrame.\")\n",
    "\n",
    "        uniques = pd.Series(df[col].unique())\n",
    "\n",
    "        if dropna:\n",
    "            uniques = uniques.dropna()\n",
    "\n",
    "        result[col] = uniques.tolist() if as_list else uniques\n",
    "\n",
    "    if len(result) == 1:\n",
    "        return next(iter(result.values()))\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "50d7b76c-237d-4400-b271-a1ee65ffd2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_columns(\n",
    "    df: pd.DataFrame,\n",
    "    columns: Union[str, List[str]],\n",
    "    ascending: Union[bool, List[bool]] = True,\n",
    "    na_position: str = 'last',\n",
    "    inplace: bool = False,\n",
    "    ignore_index: bool = False\n",
    ") -> Union[pd.DataFrame, None]:\n",
    "    \"\"\"\n",
    "    Sort a DataFrame by one or more columns.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        columns (str or list of str): Column(s) to sort by.\n",
    "        ascending (bool or list of bool): Sort order.\n",
    "        na_position (str): 'first' or 'last' (default).\n",
    "        inplace (bool): Whether to sort in place.\n",
    "        ignore_index (bool): If True, reset index in result.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame or None: Sorted DataFrame or None if inplace=True.\n",
    "    \"\"\"\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError(\"'df' must be a pandas DataFrame.\")\n",
    "    \n",
    "    if isinstance(columns, str):\n",
    "        columns = [columns]\n",
    "    elif not isinstance(columns, list) or not all(isinstance(col, str) for col in columns):\n",
    "        raise TypeError(\"'columns' must be a string or list of strings.\")\n",
    "    \n",
    "    for col in columns:\n",
    "        if col.strip() not in df.columns:\n",
    "            raise ValueError(f\"Column '{col}' not found in DataFrame.\")\n",
    "\n",
    "    if isinstance(ascending, bool):\n",
    "        ascending = [ascending] * len(columns)\n",
    "    elif isinstance(ascending, list):\n",
    "        if len(ascending) != len(columns):\n",
    "            raise ValueError(\"'ascending' list must match number of columns.\")\n",
    "        if not all(isinstance(x, bool) for x in ascending):\n",
    "            raise TypeError(\"All 'ascending' values must be bool.\")\n",
    "    else:\n",
    "        raise TypeError(\"'ascending' must be a bool or list of bools.\")\n",
    "\n",
    "    return df.sort_values(\n",
    "        by=[col.strip() for col in columns],\n",
    "        ascending=ascending,\n",
    "        na_position=na_position,\n",
    "        inplace=inplace,\n",
    "        ignore_index=ignore_index\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "82a19446-d87b-4894-8a9a-9784f5780813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_values(df):\n",
    "    \"\"\"\n",
    "    Comprehensive function to detect missing values in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing:\n",
    "        - 'summary': DataFrame with missing value statistics per column\n",
    "        - 'locations': DataFrame with row/column locations of missing values\n",
    "        - 'total_missing': Total count of missing values\n",
    "        - 'missing_percentage': Overall percentage of missing values\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input validation\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise ValueError(\"Input must be a pandas DataFrame\")\n",
    "    \n",
    "    if df.empty:\n",
    "        return {\n",
    "            'summary': pd.DataFrame(),\n",
    "            'locations': pd.DataFrame(),\n",
    "            'total_missing': 0,\n",
    "            'missing_percentage': 0.0\n",
    "        }\n",
    "    \n",
    "    # Create a copy to avoid modifying original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Replace common placeholder patterns with NaN\n",
    "    placeholder_patterns = [\n",
    "        '#########',  # Your specific pattern\n",
    "        '######### ',\n",
    "        '##############',  # Longer pattern for dates\n",
    "        'N/A',\n",
    "        'n/a',\n",
    "        'NA',\n",
    "        'na',\n",
    "        'NULL',\n",
    "        'null',\n",
    "        'Null',\n",
    "        'None',\n",
    "        'none',\n",
    "        'NONE',\n",
    "        '',\n",
    "        ' ',\n",
    "        '  ',\n",
    "        '   '\n",
    "    ]\n",
    "    \n",
    "    for pattern in placeholder_patterns:\n",
    "        df_copy = df_copy.replace(pattern, np.nan)\n",
    "    \n",
    "    # Also handle whitespace-only strings\n",
    "    df_copy = df_copy.replace(r'^\\s*$', np.nan, regex=True)\n",
    "    \n",
    "    # Calculate missing values per column\n",
    "    missing_counts = df_copy.isnull().sum()\n",
    "    missing_percentages = (missing_counts / len(df_copy)) * 100\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary = pd.DataFrame({\n",
    "        'Column': df_copy.columns,\n",
    "        'Missing_Count': missing_counts.values,\n",
    "        'Missing_Percentage': missing_percentages.values,\n",
    "        'Total_Rows': len(df_copy),\n",
    "        'Non_Missing_Count': len(df_copy) - missing_counts.values\n",
    "    })\n",
    "    \n",
    "    # Sort by missing count (descending)\n",
    "    summary = summary.sort_values('Missing_Count', ascending=False)\n",
    "    \n",
    "    # Find locations of missing values\n",
    "    locations = []\n",
    "    for col in df_copy.columns:\n",
    "        missing_indices = df_copy[df_copy[col].isnull()].index.tolist()\n",
    "        for idx in missing_indices:\n",
    "            locations.append({\n",
    "                'Row_Index': idx,\n",
    "                'Column': col,\n",
    "                'Original_Value': df.iloc[idx][col] if idx < len(df) else None\n",
    "            })\n",
    "    \n",
    "    locations_df = pd.DataFrame(locations)\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    total_cells = df_copy.shape[0] * df_copy.shape[1]\n",
    "    total_missing = missing_counts.sum()\n",
    "    overall_missing_percentage = (total_missing / total_cells) * 100 if total_cells > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'summary': summary,\n",
    "        'locations': locations_df,\n",
    "        'total_missing': int(total_missing),\n",
    "        'missing_percentage': round(overall_missing_percentage, 2),\n",
    "        'total_cells': total_cells,\n",
    "        'columns_with_missing': summary[summary['Missing_Count'] > 0]['Column'].tolist(),\n",
    "        'columns_without_missing': summary[summary['Missing_Count'] == 0]['Column'].tolist()\n",
    "    }\n",
    "\n",
    "def print_missing_report(missing_info):\n",
    "    \"\"\"\n",
    "    Print a formatted report of missing values.\n",
    "    \n",
    "    Parameters:\n",
    "    missing_info (dict): Output from find_missing_values function\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"MISSING VALUES ANALYSIS REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nOVERALL STATISTICS:\")\n",
    "    print(f\"Total cells: {missing_info['total_cells']:,}\")\n",
    "    print(f\"Total missing values: {missing_info['total_missing']:,}\")\n",
    "    print(f\"Overall missing percentage: {missing_info['missing_percentage']}%\")\n",
    "    \n",
    "    print(f\"\\nCOLUMNS WITH MISSING VALUES: {len(missing_info['columns_with_missing'])}\")\n",
    "    print(f\"COLUMNS WITHOUT MISSING VALUES: {len(missing_info['columns_without_missing'])}\")\n",
    "    \n",
    "    if not missing_info['summary'].empty:\n",
    "        print(f\"\\nMISSING VALUES BY COLUMN:\")\n",
    "        print(missing_info['summary'].to_string(index=False))\n",
    "    \n",
    "    if not missing_info['locations'].empty:\n",
    "        print(f\"\\nFIRST 10 MISSING VALUE LOCATIONS:\")\n",
    "        print(missing_info['locations'].head(10).to_string(index=False))\n",
    "    \n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "6e926621-dcf7-4453-8f02-94d5a954d944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_by_period(\n",
    "    df: pd.DataFrame,\n",
    "    date_col: str,\n",
    "    freq: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a time period column and call analyze_data grouped by it.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "\n",
    "    if freq == 'day':\n",
    "        df['day_name'] = df[date_col].dt.day_name()\n",
    "        group_col = 'day_name'\n",
    "\n",
    "    elif freq == 'week':\n",
    "        df['week_number'] = df[date_col].dt.isocalendar().week\n",
    "        df['year'] = df[date_col].dt.isocalendar().year\n",
    "        df['week_label'] = df['year'].astype(str) + '-W' + df['week_number'].astype(str)\n",
    "        group_col = 'week_label'\n",
    "\n",
    "    elif freq == 'month':\n",
    "        df['month'] = df[date_col].dt.month_name()\n",
    "        df['year'] = df[date_col].dt.year\n",
    "        df['month_label'] = df['year'].astype(str) + '-' + df['month']\n",
    "        group_col = 'month_label'\n",
    "\n",
    "    elif freq == 'quarter':\n",
    "        df['quarter_label'] = df[date_col].dt.to_period('Q').astype(str)\n",
    "        group_col = 'quarter_label'\n",
    "\n",
    "    elif freq == 'year':\n",
    "        df['year_label'] = df[date_col].dt.year\n",
    "        group_col = 'year_label'\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Frequency must be one of: 'day', 'week', 'month', 'quarter', 'year'\")\n",
    "\n",
    "    # Call your analyze_data function with the new group_by column\n",
    "    analyzed_df = analyze_data(\n",
    "        df,\n",
    "        group_by_column=group_col\n",
    "    )\n",
    "\n",
    "    return analyzed_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
